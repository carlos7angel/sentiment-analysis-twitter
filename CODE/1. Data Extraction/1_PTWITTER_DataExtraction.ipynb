{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-PTWITTER-DataExtraction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaYW76RxkmB4"
      },
      "source": [
        "# **EXTRACCIÓN DE TWEETS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tO36fm0hkaN7"
      },
      "source": [
        "# Instalamos la libreria SNscrape para la extración de los tweets e importamos algunas librerias para la manipulación de datos\n",
        "!pip3 install snscrape\n",
        "import snscrape.modules.twitter as twitter_scraper\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLnD7Dh9lLqa"
      },
      "source": [
        "# Definimos los criterios de búsqueda\n",
        "R_TOTAL = 10000\n",
        "search_texts_array = [\n",
        "    \"sputnik\",\n",
        "    \"sinopharm\",\n",
        "    \"pfizer\",\n",
        "    \"johnson & johnson\",\n",
        "    \"astrazeneca\",\n",
        "    \"vacuna china\",\n",
        "    \"vacuna rusa\",\n",
        "    \"vacuna covid\",\n",
        "    \"desconfianza vacuna\",\n",
        "    \"antivacuna\",\n",
        "    \"efecto vacuna\"\n",
        "]\n",
        "  \n",
        "coordinates_array = [\n",
        "    {'country': 'La Paz', 'latitude': -16.4942, 'longitude': -68.1475, 'radius': '50km'},\n",
        "    {'country': 'Cochabamba', 'latitude': -17.3935, 'longitude': -66.157, 'radius': '50km'},\n",
        "    {'country': 'Cobija', 'latitude': -11.0183, 'longitude': -68.7537, 'radius': '50km'},\n",
        "    {'country': 'Trinidad', 'latitude': -14.8333, 'longitude': -64.9, 'radius': '50km'},\n",
        "    {'country': 'Oruro', 'latitude': -17.9667, 'longitude': -67.1167, 'radius': '50km'},\n",
        "    {'country': 'Potosi', 'latitude': -19.5833, 'longitude': -65.75, 'radius': '50km'},\n",
        "    {'country': 'Santa Cruz', 'latitude': -17.7892, 'longitude': -63.1975, 'radius': '50km'},\n",
        "    {'country': 'Tarija', 'latitude': -21.5317, 'longitude': -64.7311, 'radius': '50km'},\n",
        "    {'country': 'Sucre', 'latitude': -19.0431, 'longitude': -65.2592, 'radius': '50km'}\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7Dvyom1lr2E"
      },
      "source": [
        "def search_by_location(search_texts, coordinates):\n",
        "    print(\"Initialize extraction processing...\")\n",
        "    count = 0\n",
        "    index = 0\n",
        "    while index < len(coordinates):\n",
        "        tweets = []\n",
        "        csv_file = open(coordinates[index]['country'].replace(\" \", \"\") + \".csv\", \"a\", encoding=\"utf-8\")\n",
        "        csv_writer = csv.writer(csv_file)\n",
        "        for index2, search in enumerate(search_texts):\n",
        "            scraper = twitter_scraper.TwitterSearchScraper(\n",
        "                search + ' since:2020-11-01 until:2021-10-31 lang:es geocode:' + str(coordinates[index]['latitude']) + ',' +\n",
        "                str(coordinates[index]['longitude']) + ',' + coordinates[index]['radius'])\n",
        "            for i, tweet in enumerate(scraper.get_items()):\n",
        "                if i > TOTAL:\n",
        "                    break\n",
        "                save_tweet(csv_writer, tweet)\n",
        "                count += 1                                    \n",
        "\n",
        "        csv_file.close()\n",
        "        index = index + 1\n",
        "    print(\"FINISHED! With {} records\".format(count))\n",
        "\n",
        "\n",
        "def search_all(search_texts):\n",
        "    print(\"Initialize extraction processing...\")\n",
        "    count = 0\n",
        "    csv_file = open(\"vacunas_bolivia.csv\", \"a\", encoding=\"utf-8\")\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "    for index, search in enumerate(search_texts):\n",
        "        scraper = twitter_scraper.TwitterSearchScraper(search + ' since:2020-11-01 until:2021-10-31 lang:es')\n",
        "        for i, tweet in enumerate(scraper.get_items()):\n",
        "            if i > TOTAL:\n",
        "              break\n",
        "            if \"Bolivia\" in tweet.user.location:\n",
        "              save_tweet(csv_writer, tweet)\n",
        "\n",
        "            count += 1\n",
        "    csv_file.close()\n",
        "    print(\"SCRIPT FINISHED!! With {} records\".format(count))\n",
        "\n",
        "\n",
        "def save_tweet(csv_writer, tweet):\n",
        "    plc_name = tweet.place.name if (tweet.place is not None) else \"\"\n",
        "    plc_country = tweet.place.country if (tweet.place is not None) else \"\"\n",
        "    plc_country_code = tweet.place.countryCode if (tweet.place is not None) else \"\"\n",
        "    coord_latitude = tweet.coordinates.latitude if (tweet.coordinates is not None) else \"\"\n",
        "    coord_longitude = tweet.coordinates.longitude if (tweet.coordinates is not None) else \"\"\n",
        "    csv_writer.writerow([\n",
        "        tweet.id,\n",
        "        tweet.date,\n",
        "        tweet.user.id,\n",
        "        tweet.user.username,\n",
        "        tweet.user.displayname,\n",
        "        tweet.user.created,\n",
        "        tweet.user.followersCount,\n",
        "        tweet.user.location,\n",
        "        coord_latitude,\n",
        "        coord_longitude,\n",
        "        plc_name,\n",
        "        plc_country,\n",
        "        plc_country_code,\n",
        "        tweet.lang,\n",
        "        tweet.conversationId,\n",
        "        tweet.content,\n",
        "        tweet.url,\n",
        "        tweet.replyCount,\n",
        "        tweet.retweetedTweet,\n",
        "        tweet.retweetCount,\n",
        "        tweet.likeCount,\n",
        "        tweet.hashtags,\n",
        "        tweet.cashtags,\n",
        "        tweet.media,\n",
        "        tweet.sourceLabel\n",
        "])\n",
        "    \n",
        "\n",
        "search_by_location(search_texts_array, coordinates_array)\n",
        "\n",
        "search_all(search_texts_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCi3Kxvzm0DB"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data_lpz = pd.read_csv(\"LaPaz.csv\", header=None, encoding = \"utf-8\")\n",
        "data_cba = pd.read_csv(\"Cochabamba.csv\", header=None, encoding = \"utf-8\")\n",
        "data_pnd = pd.read_csv(\"Cobija.csv\", header=None, encoding = \"utf-8\")\n",
        "data_tja = pd.read_csv(\"Tarija.csv\", header=None, encoding = \"utf-8\")\n",
        "data_ben = pd.read_csv(\"Trinidad.csv\", header=None, encoding = \"utf-8\")\n",
        "data_oru = pd.read_csv(\"Oruro.csv\", header=None, encoding = \"utf-8\")\n",
        "data_pts = pd.read_csv(\"Potosi.csv\", header=None, encoding = \"utf-8\")\n",
        "data_scz = pd.read_csv(\"SantaCruz.csv\", header=None, encoding = \"utf-8\")\n",
        "data_chq = pd.read_csv(\"Sucre.csv\", header=None, encoding = \"utf-8\")\n",
        "data_all = pd.read_csv(\"vacunas_bolivia.csv\", header=None, encoding = \"utf-8\")\n",
        "\n",
        "frames = [data_lpz, data_cba, data_pnd, data_tja, data_ben, data_oru, data_pts, data_scz, data_chq, data_all]\n",
        "data_project = pd.concat(frames)\n",
        "\n",
        "data_project.to_csv('rawDataset1.csv', header=False, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV-c_EXKogI8"
      },
      "source": [
        "# Definimos una funcion para buscar las respuestas y comentarios a un tweet\n",
        "def search_comments():\n",
        "    count = 0\n",
        "    df = pd.read_csv('rawDataset1.csv', header=None, encoding = \"utf-8\")\n",
        "    col_names = ['id', 'date', 'user_id', 'user_username', 'user_displayname', 'user_created',\n",
        "                 'user_followers_count', 'user_location', 'coordinates_latitude', 'coordinates_longitude',\n",
        "                 'place_name', 'place_country', 'place_country_code', 'language', 'conversation_id', 'content',\n",
        "                 'url', 'reply_count', 'retweeted_tweet', 'retweet_count', 'like_count', 'hashtags', 'cashtags',\n",
        "                 'media', 'source']\n",
        "    df.columns = col_names\n",
        "    df = df.drop_duplicates(df.columns[~df.columns.isin(['id'])], keep='first')\n",
        "    print(df.shape)\n",
        "    csv_file = open(\"rawDataset2.csv\", \"a\", encoding=\"utf-8\")\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "    for i in df.index:\n",
        "        scraper = twitter_scraper.TwitterTweetScraper(tweetId=df['id'][i], mode=twitter_scraper.TwitterTweetScraperMode.RECURSE)\n",
        "        for j, tweet in enumerate(scraper.get_items()):\n",
        "            save_tweet(csv_writer, tweet)\n",
        "            count += 1\n",
        "            print(tweet.id)\n",
        "    print('{} records saved!!!'.format(count))\n",
        "\n",
        "search_comments()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}